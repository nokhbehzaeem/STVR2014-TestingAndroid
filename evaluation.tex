\section{Evaluation}
\label{evaluation}

We evaluated \tool{} on $\NumApps$ Android apps ($3$ apps from previously studied apps and $3$ apps from the apps
we selected, as discussed in Section~\ref{study}) to answer the
following research questions:
%\begin{enumerate}
%\item 
(1) Can \tool{} find bugs in real applications?
%\item 
(2) How effective is \tool{} in terms of the ratio of real bugs to false positives (FP's)?
%\item 
(3) How compact are the test suites generated by \tool?
%\end{enumerate}

\subsection{RQ 1 and 2: Finding Real Bugs}
Given manually created GUI models, we used \tool{} to automatically
generate test suites. We first used our traversal algorithm with both optimizations, along with
the image processing oracle. Then, we automatically executed the test
suites on a rooted Android emulator (running Android 4.3 API level 18
with an Intel Atom (x86) CPU, 512 MB of SD card, and resolution
WVGA800).

\input{bugsFound}

Table~\ref{table:bugsFound} summarizes the results of finding bugs.
\emph{\#Tests} is the size of the test suite generated for each app
using our traversal algorithm with both truncation and prioritization
optimizations. Each test covers several golden edges and tests multiple features, 
thereby generating compact test suites.
\emph{\#Assertions} shows the total number of assertions in the test
suite, which is equal to the number of golden edges. \emph{\#Failures} is the number of assertions that failed. We
manually investigated the failures and identified real bugs and false
positives. Some of these bugs or false positives were revealed more
than once. Therefore, we show the number of distinct false positives
and bugs in the last two columns.

\tool{} found a total of \NumBugs{} bugs in \NumApps{} apps. These bugs included $12$ rotation bugs, $1$ killing and restarting bug, $5$ pausing and resuming bugs, and $4$ Back button bugs. Examples of the bugs are as follows.

%\noindent
{\bf Pausing and resuming bug in K9Mail}: The user finally finds an
email after searching the inbox for some time, but while reading the
email he receives a phone call (which pauses K9Mail). After the phone
call is over, K9Mail resumes, but back to the inbox, requiring to perform the
search again.
\\
\indent{\bf Killing and restarting bug in K9Mail}: The operating system decides to kill K9Mail because of low memory while the user is composing an email. K9Mail fails to save the email as a draft, deleting the contents of the email.
\\
\indent{\bf Rotation bug in Kitchen Timer}: Explained in Section~\ref{example}.
\\
\indent{\bf Rotation bug in OpenSudoku}: Rotating the device closes the custom pop up for entering numbers and discards them.
\\
\indent{\bf Rotation bug in VuDroid}: Rotation clears tab selection. 
\\
\indent{\bf Rotation bug in Nexes Manager}: If there is an empty folder which has no permission (read, write, etc.), rotating the device makes the folder icon disappear.
\\
\indent{\bf Back button bug in Kitchen Timer}: Going to sub-menus and coming back makes buttons go out of focus.

We found two of these bugs already reported and accepted in the bug repositories of the corresponding apps. All the other bugs were new.
We reported these to the respective developers and are awaiting confirmation of the bugs from them. 
%out of which we reported several. %5 bugs reported
%However, we have not yet heard back from developers regarding the reports. 

\tool{} reported a total of $9$ distinct false positives. However, $4$ of these false positives were because of an inconsistency in the Android testing instrumentation, which caused it to act differently when paused programmatically (by tests) or through the emulator GUI (when manually confirming bugs). Another $2$ false positives were because of time sensitivity of some app states. For instance, when a timer is running in Kitchen Timer, rotation changes timer values, not because there is a bug, but rather because the state of a running timer changes with time. Such time sensitivity is usually abstracted out from the app's GUI model to achieve conciseness. Another $2$ false positives manifested because if the app GUI provides a visual back button on the screen, hitting this visual button and then the hardware Back button does not take the app to the original state. The remaining $1$ false positive could be considered a bug, depending on the intent of the app designer.

\subsection{RQ 3: Compactness of Generated Test Suites}
We compared our test generation algorithm to a baseline DFS, in generating test cases that cover all golden edges. For the set of implemented features (rotation, killing and restarting, pausing and resuming, and Back button) we measured the compactness of generated test suites in terms of the number of tests and the cost of the test suite when generated by each of the following algorithms:
%\begin{enumerate}
%\item 
(1) Our basic algorithm;
%\item 
(2) Our algorithm plus the truncation optimization, which truncates test cases after the last golden edge and ignores test cases that do not cover any golden edge;
%\item 
(3) Our algorithm plus the prioritization optimization, which prioritizes golden edges while traversing the model;
%\item 
(4) Our algorithm plus both truncation and prioritization optimizations; and
%\item 
(5) A DFS algorithm starting at the root.
%\end{enumerate}

Table~\ref{table:algoEffectiveness} shows the experimental results. The cost function is calculated with both $\alpha$ and $\beta$ set to 1. As this table shows, our algorithm shows a clear improvement over DFS, in terms of the number of tests as well as the cost of executing test suites. Furthermore, truncation and prioritization improve the results when applied separately (except for Kitchen Timer, for which truncation does not improve the number of tests or cost). In some cases (Notepad, Nexes Manager, and VuDroid), truncation yields better results compared to prioritization, while in the other cases prioritization is more effective. Fortunately, the optimizations are compatible and combinable and using both of them produces even better results for all of the studied apps.

\input{algoEffectiveness}

{\bf Threats to Validity}: To minimize threats to internal validity, we automated the entire test generation and execution process and manually identified real bugs from false positives. To address external validity, we experimented with 7 previously studied apps and set forth a criterion to choose 6 other popular apps from open source repositories as discussed in Section~\ref{study}. With respect to construct validity, we strictly followed our traversal algorithm and oracle generation techniques, used well-known frameworks Robotium and JUnit, and manually investigated generated tests for some of the apps. 
%One limitation of this work is synthesizing universally applicable oracles, which gives rise to false positives.
%General purpose oracles are often either too weak to generate bugs or generate too many false positives. We are mitigating that short-coming by using domain specific knowledge. 

